---

- name: fetch the uuid of the cluster
  shell: "if [ -f /etc/ceph/monmap ]; then monmaptool --print /etc/ceph/monmap | grep fsid | cut -d' ' -f2; fi"
  changed_when: False
  register: ceph_fsid_result

- set_fact: ceph_fsid=''

- set_fact: ceph_fsid="{{ hostvars[item].ceph_fsid_result.stdout }}"
  when: hostvars[item].ceph_fsid_result.stdout != ''
  with_items: "{{ groups['ceph_monitor'] }}"

- name: generate a new uuid for the cluster
  command: uuidgen
  register: ceph_fsid_result
  when: ceph_fsid == ''
  run_once: True

- set_fact: ceph_fsid="{{ hostvars[groups['ceph_monitor'][0]].ceph_fsid_result.stdout }}"
  when: hostvars[groups['ceph_monitor'][0]].ceph_fsid_result.stdout is defined

- name: generate ceph config file
  ini_file: dest=/etc/ceph/{{ ceph_cluster_name }}.conf
    section=global
    option="{{ item.key }}"
    value="{{ item.value }}"
  with_dict:
    "fsid": "{{ ceph_fsid }}"
    "mon initial members": "{{ groups['ceph_monitor'] | join(',') }}"
    "mon host": "{% for host in groups['ceph_monitor'] %}{{ hostvars[host].ip.mgmt }}{% if not loop.last %},{% endif %}{% endfor %}"
    "auth cluster required": "cephx"
    "auth service required": "cephx"
    "auth client required": "cephx"
    "cephx require signatures": "true"
    "osd pool default size": "{{ ceph_osd_pool_default_size }}"
    "osd pool default min size": "{{ ceph_osd_pool_default_min_size }}"
    "osd pool default pg num": "{{ ceph_osd_pool_default_pg_num }}"
    "osd pool default pgp num": "{{ ceph_osd_pool_default_pgp_num }}"
    "osd crush chooseleaf type": "{{ ceph_osd_crush_chooseleaf_type }}"
    "osd max attr size": "{{ ceph_osd_max_attr_size }}"
    "osd heartbeat grace": "{{ ceph_osd_heartbeat_grace }}"
    "mon osd full ratio" : "{{ ceph_mon_osd_full_ratio }}"
    "mon osd nearfull ratio" : "{{ ceph_mon_osd_nearfull_ratio }}"
    "mon pg warn max object skew" : "{{ ceph_mon_pg_warn_max_object_skew }}"
    "mon osd down out interval" : "{{ ceph_mon_osd_down_out_interval }}"
    "mon osd downout subtree limit" : "{{ ceph_mon_osd_downout_subtree_limit }}"
    "mon osd min down reporters" : "{{ ceph_mon_osd_min_down_reporters }}"
    "mon pg warn max per osd" : "{{ ceph_mon_pg_warn_max_per_osd }}"
    "mon osd max split count" : "{{ ceph_mon_osd_max_split_count }}"
    "mon allow pool delete" : "{{ ceph_mon_allow_pool_delete }}"

- name: set up ceph public network
  ini_file: dest=/etc/ceph/{{ ceph_cluster_name }}.conf
    section=global
    option="public network"
    value="{{ ceph_public_network }}"
  when: ceph_public_network is defined

- name: set up ceph cluster network
  ini_file: dest=/etc/ceph/{{ ceph_cluster_name }}.conf
    section=global
    option="cluster network"
    value="{{ ceph_cluster_network }}"
  when: ceph_cluster_network is defined

- name: create keyrings
  command: /usr/bin/ceph-authtool --create-keyring /etc/ceph/{{ ceph_cluster_name }}.{{ item }}.keyring
    creates=/etc/ceph/{{ ceph_cluster_name }}.{{ item }}.keyring
  with_items:
    - mon
    - client.admin

# Search for existing admin and monitor keys
- name: query the keyring for the client.admin key
  command: /usr/bin/ceph-authtool /etc/ceph/{{ ceph_cluster_name }}.client.admin.keyring -n client.admin -p
  register: ceph_admin_key_results
  failed_when: ceph_admin_key_results | failed and 'not found' not in ceph_admin_key_results.stderr
  changed_when: False

- name: query the keyring for the mon. key
  command: /usr/bin/ceph-authtool /etc/ceph/{{ ceph_cluster_name }}.mon.keyring -n mon. -p
  register: ceph_mon_key_results
  failed_when: ceph_mon_key_results | failed and 'not found' not in ceph_mon_key_results.stderr
  changed_when: False

- set_fact: admin_key=''
- set_fact: mon_key=''

- set_fact: admin_key="{{ hostvars[item].ceph_admin_key_results.stdout }}"
  when: "'not found' not in hostvars[item].ceph_admin_key_results.stderr"
  with_items: "{{ groups['ceph_monitor'] }}"

- set_fact: mon_key="{{ hostvars[item].ceph_mon_key_results.stdout }}"
  when: "'not found' not in hostvars[item].ceph_mon_key_results.stderr"
  with_items: "{{ groups['ceph_monitor'] }}"

# Generate new keys if old keys are not found
- name: generate new admin key
  command: ceph-authtool --gen-print-key
  register: ceph_admin_key_results
  when: admin_key == ''
  run_once: True

- name: generate new mon. key
  command: ceph-authtool --gen-print-key
  register: ceph_mon_key_results
  when: admin_key == ''
  run_once: True

- set_fact: admin_key="{{ hostvars[groups['ceph_monitor'][0]].ceph_admin_key_results.stdout }}"
  when: hostvars[groups['ceph_monitor'][0]].ceph_admin_key_results.stdout is defined

- set_fact: mon_key="{{ hostvars[groups['ceph_monitor'][0]].ceph_mon_key_results.stdout }}"
  when: hostvars[groups['ceph_monitor'][0]].ceph_mon_key_results.stdout is defined

# Put the existing or new keys into the keyrings
- name: generate ceph.client.admin.keyring
  template: src=ceph.client.admin.keyring.j2 dest=/etc/ceph/{{ ceph_cluster_name }}.client.admin.keyring owner=root group=root mode=0600

- name: generate ceph.mon.keyring
  template: src=ceph.mon.keyring.j2 dest=/etc/ceph/{{ ceph_cluster_name }}.mon.keyring owner=ceph group=root mode=0460

# Continue with the monitor setup
- name: create the monitor map
  command: monmaptool --create --fsid "{{ ceph_fsid }}" /etc/ceph/monmap creates=/etc/ceph/monmap

- name: add the monitor hosts to the monitor map
  command: monmaptool --add {{ item }} {{ hostvars[item].ip.mgmt }} /etc/ceph/monmap
  with_items: "{{ groups['ceph_monitor'] }}"
  register: monmap_result
  failed_when: monmap_result.rc != 0 and 'map already contains' not in monmap_result.stderr
  changed_when: "'writing' in monmap_result.stdout"

- name: create the default data directories
  file: dest=/var/lib/ceph/mon/{{ ceph_cluster_name }}-{{ inventory_hostname }} state=directory owner=ceph group=root mode=0755

- name: populate the monitor daemon(s) with the monitor map and keyring
  command: ceph-mon --cluster {{ ceph_cluster_name }}
    --mkfs -i {{ inventory_hostname }}
    --monmap /etc/ceph/monmap
    --keyring /etc/ceph/{{ ceph_cluster_name }}.mon.keyring
    --setuser ceph
    --setgroup ceph

- name: ensure /etc/systemd/system/ceph-mon.service.d exists
  file: dest=/etc/systemd/system/ceph-mon.service.d state=directory owner=root group=root mode=0755

- name: install 10-name.conf override
  copy: content='[Service]\nEnvironment=CLUSTER={{ ceph_cluster_name }}\n'
        dest=/etc/systemd/system/ceph-mon.service.d/10-name.conf
        owner=root group=root mode=0644
  notify: reload systemd

- meta: flush_handlers

- name: ensure ceph monitor is started
  service: name=ceph-mon state=started enabled=yes

- name: create boostrap keys
  command: ceph-create-keys --cluster {{ ceph_cluster_name }} --id {{ inventory_hostname }}
  run_once: True
