---

- name: install ceph packages
  apt: name=ceph state=present

- name: fetch or generate the uuid of the cluster
  shell: "monmaptool --print /etc/ceph/monmap | grep fsid | cut -d' ' -f2"
  delegate_to: "{{ groups['ceph_monitor'][0] }}"
  register: ceph_fsid
  changed_when: False
  run_once: True

- name: generate ceph config file
  ini_file: dest=/etc/ceph/{{ ceph_cluster_name }}.conf
    section=global
    option="{{ item.key }}"
    value="{{ item.value }}"
  with_dict:
    "fsid": "{{ ceph_fsid.stdout }}"
    "mon initial members": "{{ groups['ceph_monitor'] | join(',') }}"
    "mon host": "{% for host in groups['ceph_monitor'] %}{{ hostvars[host].ip.mgmt }}{% if not loop.last %},{% endif %}{% endfor %}"
    "auth cluster required": "cephx"
    "auth service required": "cephx"
    "auth client required": "cephx"
    "cephx require signatures": "true"
    "osd journal size": "{{ ceph_osd_journal_size }}"
    "bluestore block size": "{{ ceph_bluestore_block_size }}"
    "filestore xattr use omap": "{{ ceph_filestore_xattr_use_omap | ternary('true','false') }}"
    "osd pool default size": "{{ ceph_osd_pool_default_size }}"
    "osd pool default min size": "{{ ceph_osd_pool_default_min_size }}"
    "osd crush chooseleaf type": "{{ ceph_osd_crush_chooseleaf_type }}"
    "osd crush update on start": "{{ ceph_osd_crush_update_on_start | ternary('true','false') }}"

- name: apply extra OSD settings
  ini_file: dest=/etc/ceph/{{ ceph_cluster_name }}.conf
    section=global
    option="{{ item.key }}"
    value="{{ item.value }}"
  with_dict: "{{ ceph_osd_extra_settings }}"

- name: set up ceph public network
  ini_file: dest=/etc/ceph/{{ ceph_cluster_name }}.conf
    section=global
    option="public network"
    value="{{ ceph_public_network }}"
  when: ceph_public_network is defined

- name: set up ceph cluster network
  ini_file: dest=/etc/ceph/{{ ceph_cluster_name }}.conf
    section=global
    option="cluster network"
    value="{{ ceph_cluster_network }}"
  when: ceph_cluster_network is defined

- name: fetch the bootstrap-osd key
  command: ceph --cluster {{ ceph_cluster_name }} auth print-key client.bootstrap-osd
  register: bootstrap_osd_key_results
  until: bootstrap_osd_key_results is success
  delay: 5
  retries: 5
  delegate_to: "{{ groups['ceph_monitor'][0] }}"
  changed_when: False
  run_once: True

- name: distribute the bootstrap-osd keyring
  copy: content="[client.bootstrap-osd]\n\tkey = {{ bootstrap_osd_key_results.stdout }}\n"
        dest=/var/lib/ceph/bootstrap-osd/{{ ceph_cluster_name }}.keyring
        owner=root group=root mode=0600

### Add path-based OSDs
- name: make sure OSD directories are exists
  file: path={{ item.path }} state=directory owner=ceph group=root mode=0700
  with_items: "{{ osd }}"
  when: item.path is defined

- name: prepare disks
  command: ceph-disk prepare
    --cluster {{ ceph_cluster_name }}
    {{ (item.id | default(None)) | ternary('--osd-id ' + item.id | default(None) | string, '') }}
    {{ (item.bluestore | default(ceph_osd_bluestore)) | ternary('--bluestore','--filestore') }}
    "{{ item.path }}"
  with_items: "{{ osd }}"
  when: item.path is defined

- name: activate disks
  command: ceph-disk activate "{{ item.path }}"
  with_items: "{{ osd }}"
  when: item.path is defined

- name: query local OSD IDs
  command: cat "{{ item.path }}/whoami"
  with_items: "{{ osd }}"
  changed_when: False
  register: osd_ids
  when: item.path is defined

- name: ensure /etc/systemd/system/ceph-osd@.service.d exists
  file: dest=/etc/systemd/system/ceph-osd@.service.d state=directory owner=root group=root mode=0755

- name: install 10-name.conf override
  copy: content='[Service]\nEnvironment=CLUSTER={{ ceph_cluster_name }}\n'
        dest=/etc/systemd/system/ceph-osd@.service.d/10-name.conf
        owner=root group=root mode=0644
  notify: reload systemd

- meta: flush_handlers

- name: ensure ceph OSD is started (systemd)
  service: name=ceph-osd@{{ item.stdout }} state=started enabled=yes
  with_items: "{{ osd_ids.results }}"
  when: item.skipped is not defined or item.skipped != True

### Add disk-based OSDs
- name: prepare disks
  command: ceph-volume lvm prepare
     --cluster {{ ceph_cluster_name }}
    {{ (item.id | default(None)) | ternary('--osd-id ' + item.id | default(None) | string, '') }}
    {{ (item.bluestore | default(ceph_osd_bluestore)) | ternary('--bluestore','--filestore') }}
    --data "{{ item.disk }}"
    {{ item.journal | default('') }}
  register: ceph_disk_status
  failed_when: ceph_disk_status.rc != 0 and ('Could not create partition 2' not in ceph_disk_status.stderr) and ('Mounted filesystem' not in ceph_disk_status.stdout)
  changed_when: ceph_disk_status.rc == 0
  with_items: "{{ osd }}"
  when: item.disk is defined

- name: activate disks
  shell: ceph-volume lvm activate $(ceph-volume lvm list /dev/vdb | awk '/osd id/{ print $3 }')
  with_items: "{{ osd }}"
  when: item.disk is defined
