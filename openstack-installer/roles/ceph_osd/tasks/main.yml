---

- name: install ceph packages
  apt: name={{ item }} state=present
  with_items:
    - ceph

- name: fetch or generate the uuid of the cluster
  shell: "monmaptool --print /etc/ceph/monmap | grep fsid | cut -d' ' -f2"
  delegate_to: "{{ groups['ceph_monitor'][0] }}"
  register: ceph_fsid
  run_once: True

- name: generate ceph config file
  ini_file: dest=/etc/ceph/ceph.conf
    section=global
    option="{{ item.key }}"
    value="{{ item.value }}"
  with_dict:
    { "fsid": "{{ ceph_fsid.stdout }}",
      "mon initial members": "{{ groups['ceph_monitor'] | join(',') }}",
      "mon host": "{% for host in groups['ceph_monitor'] %}{{ hostvars[host].ip.mgmt }}{% if not loop.last %},{% endif %}{% endfor %}",
      "auth cluster required": "cephx",
      "auth service required": "cephx",
      "auth client required": "cephx",
      "osd journal size": "{{ ceph_osd_journal_size }}",
      "filestore xattr use omap": "{{ ceph_filestore_xattr_use_omap }}",
      "osd pool default size": "{{ ceph_osd_pool_default_size }}",
      "osd pool default min size": "{{ ceph_osd_pool_default_min_size }}",
      "osd pool default pg num": "{{ ceph_osd_pool_default_pg_num }}",
      "osd pool default pgp num": "{{ ceph_osd_pool_default_pgp_num }}",
      "osd crush chooseleaf type": "{{ ceph_osd_crush_chooseleaf_type }}"
   }

- name: fetch the client.admin keyring
  fetch: src=/etc/ceph/ceph.client.admin.keyring dest=workdir/ceph.client.admin.keyring flat=yes
  delegate_to: "{{ groups['ceph_monitor'][0] }}"
  run_once: True

- name: fetch the client.bootstrap-osd keyring
  fetch: src=/var/lib/ceph/bootstrap-osd/ceph.keyring dest=workdir/bootstrap-osd.keyring flat=yes
  delegate_to: "{{ groups['ceph_monitor'][0] }}"
  run_once: True

- name: distribute the client.admin keyring
  copy: dest=/etc/ceph/ceph.client.admin.keyring src=workdir/ceph.client.admin.keyring owner=root group=root mode=0600

- name: distribute the bootstrap-osd keyring
  copy: dest=/var/lib/ceph/bootstrap-osd/ceph.keyring src=workdir/bootstrap-osd.keyring owner=root group=root mode=0600

#### Add node
- name: add the Ceph Node to the CRUSH map
  command: ceph --cluster {{ ceph_cluster_name }} osd crush add-bucket {{ inventory_hostname }} host

- name: place the Ceph Node under the root default
  command: ceph --cluster {{ ceph_cluster_name}} osd crush move {{ inventory_hostname }} root=default

### Add path-based OSDs
- name: make sure OSD directories are exists
  file: path={{ item.path }} state=directory owner=root group=root mode=0600
  with_items: osd
  when: item.path is defined

- name: prepare disks
  command: ceph-disk prepare "{{ item.path }}"
  with_items: osd
  when: item.path is defined

- name: activate disks
  command: ceph-disk activate "{{ item.path }}"
  with_items: osd
  when: item.path is defined

- name: query local OSD IDs
  command: cat "{{ item.path }}/whoami"
  with_items: osd
  register: osd_ids
  when: item.path is defined

- name: ensure ceph OSD is started
  service: name=ceph-osd state=started enabled=yes args="id={{ item.stdout }} cluster={{ ceph_cluster_name }}"
  with_items: osd_ids.results
  when: item.skipped is not defined or item.skipped != True

### Add disk-based OSDs
### TODO ###

#- name: generate OSD
#  command: ceph osd create
#  register: osd_id

#- name: create the OSD directory
#  file: dest=/var/lib/ceph/osd/{{ ceph_cluster_name }}-{{ osd_id.stdout }} state=directory owner=root group=root mode=0644

#- name: initialize the OSD data directory
#  command: ceph-osd --cluster {{ ceph_cluster_name}} -i {{ osd_id.stdout }} --mkfs --mkkey

#- name: register the OSD authentication key
#  command: ceph auth add osd.{{ osd_id.stdout }} osd 'allow *' mon 'allow profile osd' -i /var/lib/ceph/osd/{{ ceph_cluster_name}}-{{ osd_id.stdout }}/keyring

#- name: add the OSD to the CRUSH map
#  command: ceph --cluster {{ ceph_cluster_name }} osd crush add {{ osd_id.stdout }} 1.0 host={{ inventory_hostname }}

#- name: mark the config as 'done'
#  copy: content='' dest=/var/lib/ceph/osd/{{ ceph_cluster_name }}-{{ osd_id.stdout }}/upstart owner=root group=root mode=0644

#- name: ensure ceph OSD is started
#  service: name=ceph-osd state=started enabled=yes args="id={{ osd_id.stdout }} cluster={{ ceph_cluster_name }}"
